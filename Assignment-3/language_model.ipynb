{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "\n",
    "import functools \n",
    "import operator \n",
    "import random \n",
    "import pickle \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from torch.utils.data import TensorDataset \n",
    "from torch.utils.data import DataLoader \n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 \n",
    "# Training data \n",
    "keepNew = True \n",
    "with open('PTB/ptb.train.txt', mode = 'r', newline = None) as train_f: \n",
    "    train_dat = train_f.read().splitlines(keepNew) \n",
    "    \n",
    "# Validation data \n",
    "with open('PTB/ptb.valid.txt', mode = 'r', newline = None) as valid_f: \n",
    "    valid_dat = valid_f.read().splitlines(keepNew) \n",
    "\n",
    "# Testing data \n",
    "with open('PTB/ptb.test.txt', mode = 'r', newline = None) as test_f: \n",
    "    test_dat = test_f.read().splitlines(keepNew) \n",
    "    \n",
    "# Printing examples phrases \n",
    "n_examp = np.random.choice(1000, size = 6, replace = False) \n",
    "\n",
    "print(\"Example training set phrase:\", train_dat[n_examp[0]]) \n",
    "print(\"Example training set phrase:\", train_dat[n_examp[1]]) \n",
    "print(\"Example validation set phrase:\", valid_dat[n_examp[2]]) \n",
    "print(\"Example validation set phrase:\", valid_dat[n_examp[3]]) \n",
    "print(\"Example test set phrase:\", test_dat[n_examp[4]]) \n",
    "print(\"Example test set phrase:\", test_dat[n_examp[5]]) \n",
    "\n",
    "# Adding <eos> \n",
    "train_proc, valid_proc, test_proc = [], [], [] \n",
    "for l_train in train_dat: \n",
    "    l_mod = l_train.replace('\\n', '<eos>') \n",
    "    train_proc.append(l_mod) \n",
    "    \n",
    "for l_valid in valid_dat: \n",
    "    l_mod = l_valid.replace('\\n', '<eos>') \n",
    "    valid_proc.append(l_mod) \n",
    "    \n",
    "for l_train in test_dat: \n",
    "    l_mod = l_train.replace('\\n', '<eos>') \n",
    "    test_proc.append(l_mod) \n",
    "    \n",
    "# Split up each line in individual words \n",
    "train_words, valid_words, test_words = [], [], [] \n",
    "for tp in train_proc: \n",
    "    train_words.append(tp.split()) \n",
    "    \n",
    "for tp in valid_proc: \n",
    "    valid_words.append(tp.split()) \n",
    "    \n",
    "for tp in test_proc: \n",
    "    test_words.append(tp.split()) \n",
    "    \n",
    "# Flatten list of lists into a single list \n",
    "train_words = functools.reduce(operator.iconcat, train_words, []) \n",
    "valid_words = functools.reduce(operator.iconcat, valid_words, []) \n",
    "test_words = functools.reduce(operator.iconcat, test_words, []) \n",
    "    \n",
    "num_train = len(train_words) \n",
    "num_valid = len(valid_words) \n",
    "num_test = len(test_words) \n",
    "\n",
    "print(\"Number of training words:\", num_train) \n",
    "print(\"Number of validation words:\", num_valid) \n",
    "print(\"Number of testing words:\", num_test) \n",
    "\n",
    "# Building a dictionary \n",
    "all_words = train_words + valid_words + test_words \n",
    "set_words = set(all_words) \n",
    "num_unique = len(set_words) \n",
    "\n",
    "print(\"Number of unique words in training + validation + testing splits:\", num_unique) \n",
    "\n",
    "num_id = np.random.choice(num_unique, size = num_unique, replace = False) \n",
    "\n",
    "n = 0 \n",
    "unique_dict = {} \n",
    "for uw in set_words: \n",
    "    unique_dict.update({uw : num_id[n]}) \n",
    "    n += 1 \n",
    "    \n",
    "# Replacing all words in the training/validation/testing splits with their integer representation \n",
    "train_ints, valid_ints, test_ints = [], [], [] \n",
    "\n",
    "for word in train_words: \n",
    "    int_rep = unique_dict[word] \n",
    "    train_ints.append(int_rep) \n",
    "\n",
    "for word in valid_words: \n",
    "    int_rep = unique_dict[word] \n",
    "    valid_ints.append(int_rep) \n",
    "    \n",
    "for word in test_words: \n",
    "    int_rep = unique_dict[word] \n",
    "    test_ints.append(int_rep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", torch.cuda.get_device_name(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting model function \n",
    "# Credits: https://discuss.pytorch.org/t/reset-model-weights/19180/4 \n",
    "def reset_model(model):\n",
    "    for layer in model.children(): \n",
    "       if hasattr(layer, 'reset_parameters'): \n",
    "           layer.reset_parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 \n",
    "# Convert our training/validation/testing splits to Torch tensors \n",
    "train_dat = torch.tensor(train_ints) \n",
    "valid_dat = torch.tensor(valid_ints) \n",
    "test_dat = torch.tensor(test_ints) \n",
    "\n",
    "### Data loading \n",
    "batch_size = 32  \n",
    "batch_eval = 512 \n",
    "batch_test = 512  \n",
    "seq_train = 50 \n",
    "seq_valid = 50 \n",
    "seq_test = 50 \n",
    "\n",
    "s_train_l = num_train // seq_train \n",
    "s_valid_l = num_valid // seq_valid \n",
    "s_test_l = num_test // seq_test \n",
    "\n",
    "# Trim training/validation/testing data and reshape into tensor of num_sequences by sequence_length \n",
    "# Training data\n",
    "train_seq = torch.narrow(train_dat, 0, 0, seq_train * s_train_l) \n",
    "train_lab = torch.roll(train_seq, shifts = -1, dims = 0) \n",
    "\n",
    "train_seq = train_seq.reshape(s_train_l, seq_train)\n",
    "train_lab_seq = train_lab.reshape(s_train_l, seq_train)\n",
    "\n",
    "# Validation data\n",
    "valid_seq = torch.narrow(valid_dat, 0, 0, seq_valid * s_valid_l)\n",
    "valid_lab = torch.roll(valid_seq, shifts = -1, dims = 0) \n",
    "\n",
    "valid_seq = valid_seq.reshape(s_valid_l, seq_valid)\n",
    "valid_lab_seq = valid_lab.reshape(s_valid_l, seq_valid) \n",
    "\n",
    "# Testing data\n",
    "test_seq = torch.narrow(test_dat, 0, 0, seq_test * s_test_l)\n",
    "test_lab = torch.roll(test_seq, shifts = -1, dims = 0) \n",
    "\n",
    "test_seq = test_seq.reshape(s_test_l, seq_test) \n",
    "test_lab_seq = test_lab.reshape(s_test_l, seq_test) \n",
    "\n",
    "# Divide training and validation data into correct mini-batches\n",
    "num_batches = train_seq.shape[0] // batch_size \n",
    "valid_batches = valid_seq.shape[0] // batch_eval \n",
    "test_batches = test_seq.shape[0] // batch_test \n",
    "\n",
    "training_set = TensorDataset(train_seq.to(device), train_lab_seq.to(device)) \n",
    "training_loader = DataLoader(training_set, shuffle = False, batch_size = num_batches) \n",
    "\n",
    "valid_set = TensorDataset(valid_seq.to(device), valid_lab_seq.to(device)) \n",
    "valid_loader = DataLoader(valid_set, shuffle = False, batch_size = valid_batches) \n",
    "\n",
    "test_set = TensorDataset(test_seq.to(device), test_lab_seq.to(device)) \n",
    "test_loader = DataLoader(test_set, shuffle = False, batch_size = test_batches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN code  \n",
    "# Embedding parameter \n",
    "embed_dim = 500  \n",
    "\n",
    "# RNN parameters\n",
    "hidden_dim = 500    \n",
    "in_size = embed_dim \n",
    "n_layers = 2 \n",
    "\n",
    "# Vanilla RNN  \n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_embeddings, embedding_dim, hidden_dim, num_unique, drop_out):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layer = num_layers \n",
    "        self.drop_out = drop_out\n",
    "        self.num_embeddings = num_embeddings \n",
    "        self.embedding_dim = embedding_dim \n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.elman = nn.RNN(input_size, hidden_size, num_layers, dropout = drop_out, batch_first = True) \n",
    "        self.linear = nn.Linear(hidden_dim, num_unique) \n",
    "        \n",
    "    def forward(self, mod_input): \n",
    "        word_embed = self.embed(mod_input)\n",
    "        rnn_out, hidden_out = self.elman(word_embed) \n",
    "        rnn_out = self.linear(rnn_out) \n",
    "        rnn_out = rnn_out.view(-1, num_unique)\n",
    "        \n",
    "        return rnn_out, hidden_out \n",
    "    \n",
    "# LSTM \n",
    "class LSTMRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_embeddings, embedding_dim, hidden_dim, num_unique, drop_out):\n",
    "        super(LSTMRNN, self).__init__() \n",
    "        \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layer = num_layers \n",
    "        self.drop_out = drop_out \n",
    "        self.num_embeddings = num_embeddings \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout = drop_out, batch_first = True, bidirectional = False) \n",
    "        self.linear = nn.Linear(hidden_dim, num_unique) \n",
    "        \n",
    "    def forward(self, mod_input): \n",
    "        word_embed = self.embed(mod_input)\n",
    "        lstm_out, hidden_out = self.lstm(word_embed) \n",
    "        lstm_out = self.linear(lstm_out) \n",
    "        lstm_out = lstm_out.view(-1, num_unique)\n",
    "        \n",
    "        return lstm_out, hidden_out \n",
    "    \n",
    "# GRU \n",
    "class GRURNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_embeddings, embedding_dim, hidden_dim, num_unique, drop_out):\n",
    "        super(GRURNN, self).__init__() \n",
    "        \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layer = num_layers \n",
    "        self.drop_out = drop_out \n",
    "        self.num_embeddings = num_embeddings \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout = drop_out, batch_first = True, bidirectional = False) \n",
    "        self.linear = nn.Linear(hidden_dim, num_unique) \n",
    "        \n",
    "    def forward(self, mod_input): \n",
    "        word_embed = self.embed(mod_input)\n",
    "        gru_out, hidden_out = self.gru(word_embed) \n",
    "        gru_out = self.linear(gru_out) \n",
    "        gru_out = gru_out.view(-1, num_unique)\n",
    "        \n",
    "        return gru_out, hidden_out \n",
    "        \n",
    "elman_rnn = ElmanRNN(in_size, hidden_dim, n_layers, num_unique, embed_dim, hidden_dim, num_unique, 0.5) \n",
    "lstm_rnn = LSTMRNN(in_size, hidden_dim, n_layers, num_unique, embed_dim, hidden_dim, num_unique, 0.8)\n",
    "gru_rnn = GRURNN(in_size, hidden_dim, n_layers, num_unique, embed_dim, hidden_dim, num_unique, 0.8)\n",
    "\n",
    "# Selecting model \n",
    "model = gru_rnn \n",
    "\n",
    "reset_model(model) \n",
    "model.to(device) \n",
    "\n",
    "# Optimizer \n",
    "l_rate = 10.0\n",
    "sgd = optim.SGD(model.parameters(), lr = l_rate, weight_decay = 0, momentum = 0.0) \n",
    "adam = optim.Adam(model.parameters(), lr = l_rate, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.0, amsgrad = False)\n",
    "\n",
    "optimizer = sgd \n",
    "\n",
    "# Cross entropy loss \n",
    "loss = nn.CrossEntropyLoss() \n",
    "\n",
    "# Training parameters \n",
    "num_epochs = 50 \n",
    "train_epoch = np.zeros(num_epochs)\n",
    "valid_epoch = np.zeros(num_epochs) \n",
    "\n",
    "train_loss = [] \n",
    "valid_loss = [] \n",
    "test_loss = [] \n",
    "\n",
    "# Gradient clipping \n",
    "clipGrad = True  \n",
    "\n",
    "# Learning rate scheduler\n",
    "exp_sched = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.5,last_epoch = -1, verbose = True) \n",
    "cosine_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 500, eta_min = 0.1, last_epoch = -1, verbose = True) \n",
    "#plat_sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 10, threshold = 0.0001, threshold_mode = 'rel', cooldown = 0, min_lr = 0, eps = 1e-08, verbose = True)\n",
    "\n",
    "sched = cosine_sched \n",
    "useScheduler = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation on validation data \n",
    "for epoch in range(num_epochs): \n",
    "    train_losss = [] \n",
    "    valid_losss = [] \n",
    "    \n",
    "    for data, lab_train in training_loader: \n",
    "        model.train() \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        pred_out, hid_out = model(data) \n",
    "        \n",
    "        ce_loss = loss(pred_out, lab_train.flatten()) \n",
    "    \n",
    "        train_loss.append(ce_loss.item()) \n",
    "        train_losss.append(ce_loss.item()) \n",
    "    \n",
    "        ce_loss.backward() \n",
    "        \n",
    "        # Using gradient clipping \n",
    "        if clipGrad: \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm = 0.5, norm_type = 2.0) \n",
    "            \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Using learning rate scheduler \n",
    "        if useScheduler: \n",
    "            sched.step() \n",
    "        \n",
    "    train_epoch[epoch] = np.mean(train_losss) \n",
    "        \n",
    "    for data_eval, lab_eval in valid_loader: \n",
    "        model.eval() \n",
    "        with torch.no_grad(): \n",
    "            valid_preds, valid_hid = model(data_eval) \n",
    "            \n",
    "            ce_valid = loss(valid_preds, lab_eval.flatten()) \n",
    "            valid_loss.append(ce_valid.item()) \n",
    "            valid_losss.append(ce_valid.item()) \n",
    "            \n",
    "    valid_epoch[epoch] = np.mean(valid_losss) \n",
    "    \n",
    "    print(\"Epoch: %s\" % (epoch + 1)) \n",
    "\n",
    "# Evaluation on test data \n",
    "model.eval()\n",
    "for data_test, lab_test in test_loader:\n",
    "    with torch.no_grad(): \n",
    "            test_preds, test_hid = model(data_test) \n",
    "            \n",
    "            ce_test = loss(test_preds, lab_test.flatten()) \n",
    "            test_loss.append(ce_test.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize = (6.4, 4.8)) \n",
    "train, = plt.plot(train_epoch, 'r') \n",
    "valid, = plt.plot(valid_epoch, 'b') \n",
    "plt.xlabel(\"Epoch\") \n",
    "plt.ylabel(\"Loss\") \n",
    "plt.title(\"Averaged training and validation loss\") \n",
    "plt.legend([train, valid], ['Train loss', 'Validation loss']) \n",
    "plt.annotate(\"Final average training loss: %s\" % (train_epoch[-1]) ,xycoords = 'figure fraction', xy = (0.25,0.55)) \n",
    "plt.annotate(\"Final average validation loss: %s\" % (valid_epoch[-1]), xycoords = 'figure fraction', xy = (0.25,0.50)) \n",
    "plt.savefig(\"train_valid_epoch\", dpi = 500) \n",
    "print(\"Final average training loss: %s.\" % train_epoch[-1]) \n",
    "print(\"Final average validation loss: %s.\" % valid_epoch[-1]) \n",
    "print() \n",
    "\n",
    "plt.figure(2, figsize = (6.4, 4.8)) \n",
    "train_it, = plt.plot(train_loss, 'r') \n",
    "valid_it, = plt.plot(valid_loss, 'b') \n",
    "plt.xlabel(\"Iteration\") \n",
    "plt.ylabel(\"Loss\") \n",
    "plt.title(\"Training and validation loss\") \n",
    "plt.legend([train_it, valid_it], ['Train loss', 'Validation loss']) \n",
    "plt.annotate(\"Final training loss: %s\" % (train_loss[-1]) ,xycoords = 'figure fraction', xy = (0.25,0.55)) \n",
    "plt.annotate(\"Final validation loss: %s\" % (valid_loss[-1]), xycoords = 'figure fraction', xy = (0.25,0.50)) \n",
    "#plt.savefig(\"train_valid_loss_iter\", dpi = 500) \n",
    "print(\"Final training loss: %s.\" % train_loss[-1]) \n",
    "print(\"Final validation loss: %s.\" % valid_loss[-1]) \n",
    "print() \n",
    "\n",
    "plt.figure(3, figsize = (6.4, 4.8)) \n",
    "train_fin_ep, = plt.plot(train_losss, 'r') \n",
    "valid_fin_ep, = plt.plot(valid_losss, 'b') \n",
    "plt.xlabel(\"Iteration\") \n",
    "plt.ylabel(\"Loss\") \n",
    "plt.title(\"Training and validation loss final epoch\") \n",
    "plt.legend([train_fin_ep, valid_fin_ep], ['Train loss', 'Validation loss']) \n",
    "plt.annotate(\"Training loss final epoch: %s\" % (train_losss[-1]) ,xycoords = 'figure fraction', xy = (0.25,0.55)) \n",
    "plt.annotate(\"Validation loss final epoch: %s\" % (valid_losss[-1]), xycoords = 'figure fraction', xy = (0.25,0.50)) \n",
    "#plt.savefig(\"train_valid_final_epoch\", dpi = 500) \n",
    "\n",
    "perplexity = np.exp(np.mean(test_loss)) \n",
    "print(\"Test perplexity: %s.\" % (perplexity)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample phrases \n",
    "dictionary = unique_dict # number of unique words/tokens is 10000 \n",
    "dict_keys = list(dictionary.keys())\n",
    "dict_values = list(dictionary.values()) \n",
    "\n",
    "# Words \n",
    "w_1 = \"there\" \n",
    "i_1 = dictionary[w_1]\n",
    "w_2 = \"is\" \n",
    "i_2 = dictionary[w_2] \n",
    "w_3 = \"a\"\n",
    "i_3 = dictionary[w_3] \n",
    "w_4 = \"why\" \n",
    "i_4 = dictionary[w_4] \n",
    "w_5 = \"on\"\n",
    "i_5 = dictionary[w_5] \n",
    "\n",
    "# Input sequence \n",
    "input_tens = torch.randint(num_unique, (2, 1)).long().to(device) \n",
    "input_tens[0] = i_1\n",
    "print(\"Input\", input_tens)\n",
    "\n",
    "inputs_list = input_tens.tolist()\n",
    "print(\"List input\", inputs_list)\n",
    "\n",
    "softmax_layer = nn.Softmax(dim = 1) \n",
    "num_words = 5 # how many words to predict \n",
    "model.eval() \n",
    "with torch.no_grad(): \n",
    "    for n in range(num_words): \n",
    "        out, hidden = model(input_tens) \n",
    "        probs = softmax_layer(out) \n",
    "    \n",
    "        maxv, maxi = torch.max(probs, dim = 1)\n",
    "        sortedv, sortedi = -np.sort(-maxv.cpu().numpy()), -np.sort(-maxi.cpu().numpy())\n",
    "        \n",
    "        next_word = sortedi[0]\n",
    "        \n",
    "        inputs_list.append([next_word])\n",
    "        \n",
    "        input_tens = input_tens.tolist() \n",
    "        input_tens.append([next_word])\n",
    "        input_tens = torch.tensor(input_tens).long().to(device)\n",
    "\n",
    "    \n",
    "print(inputs_list)\n",
    "output_sentence = [] \n",
    "\n",
    "for i in inputs_list: \n",
    "    word_i = dict_keys[dict_values.index(i[0])]\n",
    "    output_sentence.append(word_i)\n",
    "\n",
    "print(output_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_cuda]",
   "language": "python",
   "name": "conda-env-torch_cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
