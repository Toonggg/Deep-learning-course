{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "from load_mnist import load_mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training and testing data\n",
    "xtrain, ytrain, xtest, ytest = load_mnist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset function\n",
    "def reset_model(model):\n",
    "    for layer in model.children():\n",
    "       if hasattr(layer, 'reset_parameters'): \n",
    "           layer.reset_parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions of training and testing data\n",
    "M = ytrain.shape[1] \n",
    "p = xtrain.shape[1] \n",
    "\n",
    "ntrain = xtrain.shape[0] \n",
    "ntest = xtest.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting training and testing data to torch tensors \n",
    "xtra_torch = torch.tensor(xtrain).float()\n",
    "ytra_torch = torch.tensor(ytrain).float()\n",
    "xtes_torch = torch.tensor(xtest).float()\n",
    "ytes_torch = torch.tensor(ytest).float() \n",
    "\n",
    "# Converting our flat vectors to images for the CNNs\n",
    "xtra_conv = torch.tensor(xtrain).reshape(ntrain, 28, 28).float().unsqueeze(1)\n",
    "ytra_conv = torch.tensor(ytrain).float()\n",
    "xtes_conv = torch.tensor(xtest).reshape(ntest, 28, 28).float().unsqueeze(1)\n",
    "ytes_conv = torch.tensor(ytest).float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", torch.cuda.get_device_name(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN\n",
    "num_batch = 1000    \n",
    "training_data = TensorDataset(xtra_torch.to(device), ytra_torch.to(device)) \n",
    "train_dat_fcn = DataLoader(training_data, shuffle = True, batch_size = num_batch) \n",
    "\n",
    "xte = xtes_torch.to(device) \n",
    "yte = ytes_torch.to(device) \n",
    "\n",
    "# CNN\n",
    "train_dat_im = TensorDataset(xtra_conv.to(device), ytra_conv.to(device))\n",
    "train_dat_cnn = DataLoader(train_dat_im, shuffle = True, batch_size = num_batch)\n",
    "\n",
    "xte_im = xtes_conv.to(device) \n",
    "yte_im = ytes_conv.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer network \n",
    "model_2L = nn.Sequential(nn.Linear(p, 100), nn.ReLU(), nn.Linear(100, M)) \n",
    "\n",
    "# 4-layer network \n",
    "model_4L = nn.Sequential(nn.Linear(p, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, M)) \n",
    "\n",
    "# Convolutional network \n",
    "conv_mod = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3), stride = 1, padding = 1), nn.ReLU(), nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                          nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), stride = 1, padding = 1), nn.ReLU(), nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                         nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3,3), stride = 1, padding = 1), nn.ReLU(), nn.Flatten(), nn.Linear(32 * 7 * 7, M)) \n",
    "\n",
    "conv_swap = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3), stride = 1, padding = 1), nn.MaxPool2d(kernel_size = 2, stride = 2), nn.ReLU(),\n",
    "                          nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), stride = 1, padding = 1), nn.MaxPool2d(kernel_size = 2, stride = 2), nn.ReLU(), \n",
    "                        nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3,3), stride = 1, padding = 1), nn.ReLU(), nn.Flatten(), nn.Linear(32 * 7 * 7, M)) \n",
    "\n",
    "conv_swap_htan = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3), stride = 1, padding = 1), nn.MaxPool2d(kernel_size = 2, stride = 2), nn.Tanh(),\n",
    "                          nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), stride = 1, padding = 1), nn.MaxPool2d(kernel_size = 2, stride = 2), nn.Tanh(), \n",
    "                        nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3,3), stride = 1, padding = 1), nn.Tanh(), nn.Flatten(), nn.Linear(32 * 7 * 7, M))\n",
    "\n",
    "conv_mod_bn = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3), stride = 1, padding = 1), nn.BatchNorm2d(8), nn.ReLU(), nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                          nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), stride = 1, padding = 1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                        nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3,3), stride = 1, padding = 1), nn.BatchNorm2d(32), nn.ReLU(), nn.Flatten(), nn.Linear(32 * 7 * 7, M)) \n",
    "\n",
    "conv_mod_bn_dropout = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3), stride = 1, padding = 1), nn.BatchNorm2d(8), nn.ReLU(), nn.Dropout2d(0.5), nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                          nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), stride = 1, padding = 1), nn.BatchNorm2d(16), nn.ReLU(), nn.Dropout2d(0.5), nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                        nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3,3), stride = 1, padding = 1), nn.BatchNorm2d(32), nn.ReLU(), nn.Dropout2d(0.5), nn.Flatten(), nn.Linear(32 * 7 * 7, M)) \n",
    "\n",
    "conv_mod_bn_selu = nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3), stride = 1, padding = 1), nn.SELU(), nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                          nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), stride = 1, padding = 1), nn.SELU(), nn.MaxPool2d(kernel_size = 2, stride = 2), \n",
    "                        nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3,3), stride = 1, padding = 1), nn.SELU(), nn.Flatten(), nn.Linear(32 * 7 * 7, M)) \n",
    "\n",
    "\n",
    "# Selecting model \n",
    "mod = conv_mod  \n",
    "\n",
    "# Resetting model when we switch architectures \n",
    "reset_model(mod) \n",
    "\n",
    "# Moving model to GPU \n",
    "mod.to(device) \n",
    "\n",
    "# Check dimensions through network \n",
    "check_dims = False \n",
    "if check_dims == True: \n",
    "    X = torch.rand(size = (10, 1, 28, 28)).to(device) \n",
    "    for layer in mod: \n",
    "        X = layer(X) \n",
    "        print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function \n",
    "loss = F.cross_entropy \n",
    "it = 0 \n",
    "loss_train , acc_train = [], [] \n",
    "loss_test, acc_test = [], [] \n",
    "l_rate = 0.05\n",
    "\n",
    "# Vanilla SGD optimizer \n",
    "sgd_opt = optim.SGD(mod.parameters(), lr = l_rate) \n",
    "\n",
    "# Adam optimizer\n",
    "adam_opt = optim.Adam(mod.parameters(), lr = 0.001 , betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0, amsgrad = False) \n",
    "\n",
    "# Exercise 1.5 - choosing three different optimizers \n",
    "# Adadelta optimizer\n",
    "adad_opt = optim.Adadelta(mod.parameters(), lr = 1.0, rho = 0.9, eps = 1e-06, weight_decay = 0) \n",
    " \n",
    "# Adagrad optimizer\n",
    "adag_opt = optim.Adagrad(mod.parameters(), lr = 0.01, lr_decay = 0, weight_decay = 0, initial_accumulator_value = 0, eps = 1e-10) \n",
    "\n",
    "# Adamax optimizer\n",
    "adamax_opt = optim.Adamax(mod.parameters(), lr = 0.002, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0) \n",
    "\n",
    "# ASGD optimizer\n",
    "asgd_opt = optim.ASGD(mod.parameters(), lr = 0.01, lambd = 0.0001, alpha = 0.75, t0 = 1000000.0, weight_decay = 0) \n",
    "\n",
    "# RMSprop optimizer \n",
    "rmsp_opt = optim.RMSprop(mod.parameters(), lr = 0.01, alpha = 0.99, eps = 1e-08, weight_decay = 0, momentum = 0, centered = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting optimizer\n",
    "optr = adam_opt\n",
    "full_n = True  \n",
    "\n",
    "if full_n == False: \n",
    "    training_dat = train_dat_fcn \n",
    "    xt = xte \n",
    "    yt = yte \n",
    "else:\n",
    "    training_dat = train_dat_cnn \n",
    "    xt = xte_im \n",
    "    yt = yte_im "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization loop\n",
    "num_epochs = 50\n",
    "\n",
    "y_predictions = []\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    mod.train() \n",
    "    for xbatch, ybatch in training_dat: \n",
    "        optr.zero_grad() \n",
    "        prediction = mod(xbatch) \n",
    "        pred = torch.argmax(prediction, dim = 1) \n",
    "        \n",
    "        true_lab = torch.argmax(ybatch, dim = 1) \n",
    "        \n",
    "        ce_loss = loss(prediction, true_lab) \n",
    "        \n",
    "        acc_train.append(100 * (1/num_batch) * torch.sum(pred == true_lab, dim = 0).item()) \n",
    "\n",
    "        loss_train.append(ce_loss.item()) \n",
    "        \n",
    "        ce_loss.backward() \n",
    "        optr.step() \n",
    "        \n",
    "        mod.eval() \n",
    "        with torch.no_grad(): \n",
    "            ytrue_lab = torch.argmax(yt, dim = 1) \n",
    "            prediction_test = mod(xt) \n",
    "            pred_test = torch.argmax(prediction_test, dim = 1) \n",
    "            \n",
    "            ce_test = loss(prediction_test, ytrue_lab) \n",
    "            loss_test.append(ce_test.item()) \n",
    "            acc_test.append(100 * (1/ntest) * torch.sum(pred_test == ytrue_lab, dim = 0).item()) \n",
    "            \n",
    "            # Saving final predictions \n",
    "            if epoch == (num_epochs - 1): \n",
    "                y_predictions = mod(xt) \n",
    "            \n",
    "        it += 1 \n",
    "    print(\"Epoch %s/%s\" % (epoch + 1, num_epochs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1) \n",
    "loss_tra, = plt.plot(loss_train, 'r') \n",
    "loss_tes, = plt.plot(loss_test, 'b') \n",
    "plt.title(\"Training and testing loss\") \n",
    "plt.xlabel(\"Iteration\") \n",
    "plt.ylabel(\"Loss\") \n",
    "plt.legend([loss_tra, loss_tes], ['Train loss', 'Test loss']) \n",
    "plt.annotate(\"Final train loss: %s\" % (loss_train[-1]) ,xycoords = 'figure fraction', xy = (0.2,0.5)) \n",
    "plt.annotate(\"Final test loss: %s\" % (loss_test[-1]), xycoords = 'figure fraction', xy = (0.2,0.55)) \n",
    "print(\"Final training loss: %s.\" % loss_train[-1]) \n",
    "print(\"Final testing loss: %s.\" % loss_test[-1]) \n",
    "plt.savefig(\"loss_plots\", dpi = 500) \n",
    "\n",
    "plt.figure(2) \n",
    "acc_tra, = plt.plot(acc_train, 'r') \n",
    "acc_tes, = plt.plot(acc_test, 'b') \n",
    "plt.title(\"Training and testing accuracy\") \n",
    "plt.xlabel(\"Iteration\") \n",
    "plt.ylabel(\"Accuracy in %\") \n",
    "plt.legend([acc_tra, acc_tes], ['Train accuracy', 'Test accuracy'])\n",
    "plt.annotate(\"Final train accuracy: %s%%\" % (acc_train[-1]) ,xycoords = 'figure fraction', xy = (0.2,0.5))\n",
    "plt.annotate(\"Final test accuracy: %s%%\" % (acc_test[-1]), xycoords = 'figure fraction', xy = (0.2,0.55))\n",
    "print() \n",
    "print(\"Final training accuracy: %s%%.\" % acc_train[-1])\n",
    "print(\"Final testing accuracy: %s%%.\" % acc_test[-1]) \n",
    "plt.savefig(\"acc_plots\", dpi = 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data and labels from the GPU \n",
    "y_test = yte_im.data.cpu() \n",
    "\n",
    "# Remove singleton dimension \n",
    "y_test = np.squeeze(y_test, axis = 1) \n",
    "ytrue = torch.argmax(y_test, dim = 1) \n",
    "\n",
    "ypreds = y_predictions.data.cpu() \n",
    "ypredslab = torch.argmax(ypreds, dim = 1) \n",
    "\n",
    "cf_mat = confusion_matrix(ytrue, ypredslab, normalize = None) \n",
    "\n",
    "mnist_classes = np.arange(10) \n",
    "mnist_lab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] \n",
    "\n",
    "if True: \n",
    "    plt.figure(3) \n",
    "    cmat = sns.heatmap(cf_mat, cmap = 'Greens', annot = True, fmt = \"d\", cbar_kws = {'label':'Number'}) # use .3f for floating-point data \n",
    "    cmat.set_xticklabels(mnist_lab, rotation = 'horizontal') \n",
    "    cmat.set_yticklabels(mnist_lab, rotation = 'horizontal') \n",
    "    plt.xticks(mnist_classes, mnist_lab) \n",
    "    plt.yticks(mnist_classes, mnist_lab) \n",
    "    plt.savefig(\"confusion_mat\", dpi = 500) \n",
    "\n",
    "# Showing 3 examples of wrong classifications \n",
    "wrong_examples = (ytrue != ypredslab) \n",
    "\n",
    "wrong_preds = np.squeeze(xtes_conv[wrong_examples]) \n",
    "wrong_labs = ypredslab[wrong_examples] \n",
    "actual_labs = torch.argmax(ytes_conv[wrong_examples], dim = 1)\n",
    "\n",
    "ind = np.random.choice(a = wrong_labs.shape[0], size = 4, replace = True)\n",
    "\n",
    "fig, ax = plt.subplots(2 , 2) \n",
    "for a in fig.axes: \n",
    "    a.set_xticks([]) \n",
    "    a.set_yticks([]) \n",
    "    \n",
    "ax[0,0].imshow(wrong_preds[ind[0], :, :], cmap = 'gray') \n",
    "ax[0,1].imshow(wrong_preds[ind[1], :, :], cmap = 'gray') \n",
    "ax[1,0].imshow(wrong_preds[ind[2], :, :], cmap = 'gray') \n",
    "ax[1,1].imshow(wrong_preds[ind[3], :, :], cmap = 'gray') \n",
    "plt.savefig(\"wrong_preds\", dpi = 500) \n",
    "\n",
    "for i in ind: \n",
    "    print(\"Correct label: %d\" % actual_labs[i].item()) \n",
    "    print(\"Predicted label: %d\" % wrong_labs[i].item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_cuda]",
   "language": "python",
   "name": "conda-env-torch_cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
